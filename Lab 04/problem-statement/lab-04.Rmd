---
title: "Lab 4: Do a teacherâ€™s expectations influence student achievement?"
author: "Lei Qian and Rebecca C. Steorts"
date: "August 24, 2020"
output: 
     pdf_document:
      includes: 
          in_header: custom2.tex
---

# Agenda

Do a teacher's expectations influence student achievement? In a famous study, Rosenthal and Jacobson (1968) performed an experiment in a California elementary school to try to answer this question. At the beginning of the year, all students were given an IQ test.  For each class, the researchers randomly selected around 20\% of the students, and told the teacher that these students were ``spurters'' that could be expected to perform particularly well that year. (This was not based on the test---the spurters were randomly chosen.) At the end of the year, all students were given another IQ test. The change in IQ score for the first-grade students was:\footnote{The original data is not available. This data is from the \texttt{ex1321} dataset of the \texttt{R} package \texttt{Sleuth3}, which was constructed to match the summary statistics and conclusions of the original study.}

\begin{quote}
spurters (S)\\
$x$ = (18, 40, 15, 17, 20, 44, 38)
\end{quote}
\begin{quote}
controls (C)\\
$y$ = (--4, 0, --19, 24, 19, 10, 5, 10, 29, 13, --9, --8, 20, --1, 12, 21, --7, 14,
     13, 20, 11, 16, 15, 27, 23, 36, --33, 34, 13, 11, --19, 21, 6, 25, 30,
     22, --28, 15, 26, --1, --2, 43, 23, 22, 25, 16, 10, 29)
\end{quote}

1. Plot histograms for the change in IQ score for the two groups. Report your findings. 

2. How strongly does this data support the hypothesis that the teachers? expectations caused the spurters to perform better than their classmates? 
IQ tests are purposefully calibrated to make the scores normally distributed, so it makes sense to use a normal model here:
\begin{align*}
\text{spurters: } X_1,\dotsc,X_{n_S}\iid \N(\mu_S,\lambda_S^{-1})\\
\text{controls: } Y_1,\dotsc,Y_{n_C}\iid \N(\mu_C,\lambda_C^{-1}).
\end{align*}
We are interested in the difference between the means---in particular, is $\mu_S>\mu_C$?
We don't know the standard deviations $\sigma_S=\lambda_S^{-1/2}$ and $\sigma_C=\lambda_C^{-1/2}$, and the sample seems too small to estimate them very well.

it is easy using a Bayesian approach: we just need to compute the posterior probability that $\mu_S>\mu_C$:
$$ \Pr(\bm\mu_S > \bm\mu_C \mid x_{1:n_S},y_{1:n_C}). $$

Let's use independent NormalGamma priors:
\begin{align*}
\text{spurters: } (\bm\mu_S,\bm\lambda_S) \sim \NormalGamma(m,c,a,b)\\
\text{controls: } (\bm\mu_C,\bm\lambda_C) \sim \NormalGamma(m,c,a,b)
\end{align*}

with the following hyperparameter settings, based on subjective prior knowledge:
\begin{itemize}
\item $m = 0$ (Don't know whether students will improve or not, on average.)
\item $c = 1$ (Unsure about how big the mean change will be---prior certainty in our choice of $m$ assessed to be equivalent to one datapoint.)
\item $a = 1/2$ (Unsure about how big the standard deviation of the changes will be.)
\item $b = 10^2 a$ (Standard deviation of the changes expected to be around $10 = \sqrt{b/a} = \E(\lambda)^{-1/2}$.)
\end{itemize}

The updated posterior parameters are
\begin{align*}
\text{for spurters:} & \\
    M & =\frac{1\cdot 0 + 7 \cdot 27.43}{1 + 7} = 24.0\\
    C & = 1 + 7 = 8\\
    A & = 1/2 + 7/2 = 4\\ 
    B &= 100/2 + \tfrac{1}{2}\cdot 7\cdot 11.66^2 + \tfrac{1}{2}\frac{1\cdot 7}{1 + 7}(27.43 - 0)^2 = 855.0 \\
\text{for controls:} & \\
    M & =\frac{1\cdot 0 + 48\cdot 12.04}{1 + 48} = 11.8\\
    C & = 1 + 48 = 49\\
    A & = 1/2 + 48/2 = 24.5\\
    B &= 100/2 + \tfrac{1}{2}\cdot 48\cdot 16.10^2 + \tfrac{1}{2}\frac{1\cdot 48}{1 + 48}(12.04 - 0)^2 = 6344.0
\end{align*}

which implies that 

\begin{align*}
\bm\mu_S,\bm\lambda_S\mid x_{1:n_S} &\,\sim\,\NormalGamma(24.0,8,4,855.0)\\
\bm\mu_C,\bm\lambda_C\mid y_{1:n_C} &\,\sim\,\NormalGamma(11.8,49,24.5,6344.0).
\end{align*}


3. Based on the calculations from the previous task, provide a scatterplot of samples from the posterior distributions for the two groups. Your scatterplot should look similar to Figure \ref{figure:pygmalion-posteriors}. What are your conclusions? 

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/pygmalion-posteriors.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Samples of $(\mu,\sigma)$ from the posteriors for the two groups.}
  \label{figure:pygmalion-posteriors}
\end{figure}

4.  Now, we can answer our original question: ``What is the posterior probability that $\mu_S>\mu_C$?'' 

The easiest way to do this is to take a bunch of samples from each of the posteriors, and see what fraction of times we have $\mu_S>\mu_C$. This is an example of a Monte Carlo approximation (much more to come on this in the future). 

To do this, we draw $N=10^6$ samples from each posterior:
\begin{align*}
&(\mu_S^{(1)},\lambda_S^{(1)}),\dotsc,(\mu_S^{(N)},\lambda_S^{(N)})\sim \NormalGamma(24.0,8,4,855.0)\\
&(\mu_C^{(1)},\lambda_C^{(1)}),\dotsc,(\mu_C^{(N)},\lambda_C^{(N)})\sim\NormalGamma(11.8,49,24.5,6344.0)
\end{align*}
and obtain the approximation
\begin{align*}
\Pr(\bm\mu_S > \bm\mu_C \mid x_{1:n_S},y_{1:n_C}) 
\approx \frac{1}{N} \sum_{i = 1}^N \I\big(\mu_S^{(i)}>\mu_C^{(i)}\big) =  \;\;??
\end{align*}

Interpret the posterior probability that you compute above. 

5. Let's return back to the prior assumptions. There are a few ways that you can check that the prior conforms with our prior beliefs. Let's go back and checks these. Draw some samples from the prior and look at them---this is probably the best general strategy. See Figure \ref{figure:pygmalion-prior}. It's also a good idea to look at sample hypothetical datasets $X_{1:n}$ drawn using these sampled parameter values. 

Please replicate a plot similar to Figure \ref{figure:pygmalion-prior} and report your findings. 

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/pygmalion-prior.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Samples of $(\mu,\sigma)$ from the prior.}
  \label{figure:pygmalion-prior}
\end{figure}





# Loading Packages
     
Let's first load packages that we'll need in this assignment and also load the data. 
```{r, results = "hide", echo = FALSE, message = FALSE}
## don't use data in package, use data on lab assignment #
library(plyr)
library(ggplot2)
library(dplyr)
library(xtable)
library(reshape)
```
 
```{r}
set.seed(123)
# input data
# spurters
x = c(18, 40, 15, 17, 20, 44, 38)
# control group
y = c(-4, 0, -19, 24, 19, 10, 5, 10,
      29, 13, -9, -8, 20, -1, 12, 21,
      -7, 14, 13, 20, 11, 16, 15, 27,
      23, 36, -33, 34, 13, 11, -19, 21,
      6, 25, 30,22, -28, 15, 26, -1, -2,
      43, 23, 22, 25, 16, 10, 29)
# store data in data frame 
iqData = data.frame(Treatment = c(rep("Spurters", length(x)), 
                                  rep("Controls", length(y))),
                                  Gain = c(x, y))
```

    
### Task 1

Plot histograms for the change in IQ score for the two groups. Report your findings.

```{r}
xLimits = seq(min(iqData$Gain) - (min(iqData$Gain) %% 5),
              max(iqData$Gain) + (max(iqData$Gain) %% 5),
              by = 5)

ggplot(data = iqData, aes(x = Gain, fill = Treatment, colour = I("black"))) + 
  geom_histogram(position = "dodge", alpha = 0.5, breaks = xLimits, closed = "left")+
  scale_x_continuous(breaks = xLimits, 
                     expand = c(0,0))+ 
  scale_y_continuous(expand = c(0,0), 
breaks = seq(0, 10, by = 1))+
  ggtitle("Histogram of Change in IQ Scores") + labs(x = "Change in IQ Score", 
                                                     fill = "Group") + 
                                               theme(plot.title = element_text(hjust = 0.5))  
```

From the histograms, I know that the randomly selected "spurters" group has a different distribution than the "controls" group. This could indicate that teachers being told that a specific group of students is expected to perform particularly well will pay more attention and time on that group and resulting in more improvement over the year.    

### Task 2

How strongly does this data support the hypothesis that the teachers? expectations caused the spurters to perform better than their classmates?  IQ tests are purposefully calibrated to make the scores normally distributed, so it makes sense to use a normal model here:
$$
\begin{aligned}
X_1,\ldots, X_{n_s}\mid \mu_s, \lambda_s^{-1}  &\stackrel{iid}{\sim}\text{Normal}(\mu_S,\lambda_S^{-1}) \\
Y_1,\ldots,Y_{n_C} \mid \mu_c, \lambda_c^{-1} & \stackrel{iid}{\sim} \text{Normal}(\mu_C,\lambda_C^{-1}).
\end{aligned}
$$

We are interested in the difference between the means---in particular, is $\mu_S>\mu_C$?
We don't know the standard deviations $\sigma_S=\lambda_S^{-1/2}$ and $\sigma_C=\lambda_C^{-1/2}$, and the sample seems too small to estimate them very well.

It is easy using a Bayesian approach. We just need to compute the posterior probability that $\mu_S>\mu_C$: 
$$ \Pr(\mu_S > \mu_C \mid x_{1:n_S},y_{1:n_C}). $$

Let's assume independent Normal-Gamma priors:

$$
\begin{aligned}
\text{spurters: } (\mu_S,\lambda_S) \sim \text{NormalGamma}(m,c,a,b)\\
\text{controls: } (\mu_C,\lambda_C) \sim \text{NormalGamma}(m,c,a,b)
\end{aligned}
$$
with the following hyperparameter settings, based on subjective prior knowledge:

- $m = 0$ (Don't know whether students will improve or not, on average.)
-  $c = 1$ (Unsure about how big the mean change will be---prior certainty in our choice of $m$ assessed to be equivalent to one datapoint.)
- $a = 1/2$ (Unsure about how big the standard deviation of the changes will be.)
-  $b = 10^2 a$ (Standard deviation of the changes expected to be around $10 = \sqrt{b/a} = E(\lambda)^{-1/2}$.)

In order to solve this task, let's recall that the likelihoods are gaussians:

## Likelihood Functions

$$
\begin{aligned}
p(X_1...X_{n_s}|\mu_s, \lambda_s^{-1}) &= \prod_{i=1}^{n_s} \frac{1}{\sqrt{2{\sigma_s}^2 \pi}} e^{-\frac{(x_i-{\mu_s})^2}{2{\sigma_s}^2}} = \prod_{i=1}^{n_s} \frac{1}{\sqrt{2{\lambda_s}^{-1} \pi}} e^{-\frac{{\lambda_s}(x_i-{\mu_s})^2}{2}}  \\
p(Y_1...Y_{n_c}|\mu_c, \lambda_c^{-1}) &= \prod_{i=1}^{n_c} \frac{1}{\sqrt{2{\sigma_c}^2 \pi}} e^{-\frac{(y_i-{\mu_c})^2}{2{\sigma_c}^2}} = \prod_{i=1}^{n_c} \frac{1}{\sqrt{2{\lambda_c}^{-1} \pi}} e^{-\frac{{\lambda_c}(y_i-{\mu_c})^2}{2}} \\
\end{aligned}
$$

## Priors

Note that the priors are Normal-Gammas. 

$$
\begin{aligned}
p(\mu_s, \lambda_s|m,c,a,b) &= \frac{b^a \sqrt{c}}{\Gamma(a) \sqrt{2\pi}} \lambda_s^{a-0.5}e^{-b \lambda_s} e^{-\frac{c \lambda_s (\mu_s - m)^2}{2}}  \\
p(\mu_c, \lambda_c|m,c,a,b) &= \frac{b^a \sqrt{c}}{\Gamma(a) \sqrt{2\pi}} \lambda_c^{a-0.5}e^{-b \lambda_c} e^{-\frac{c \lambda_c (\mu_c - m)^2}{2}} \\
\end{aligned}
$$

## Posterior Distribution

Recall that from the course notes, that the posterior distribution of the likelihood and prior is an updated Normal Gamma, which has the following form: 

$$
\begin{aligned}
(\mu_s, \lambda_s)|x_{1:n_s} \sim & \text{NormalGamma}\left(m' = \frac{cm + n_s\bar{x}}{c + n_s}, c' = c + n_s, a' = a + \frac{n_s}{2}, b' = b + \frac{1}{2}\sum_{i=1}^{n_s}(x_i - \bar{x})^2 + \frac{n_s c}{c + n_s}\frac{(\bar{x}-m)^2}{2}\right) \\ 
&= \text{NormalGamma}(24, 8, 4, 855) \\
(\mu_c, \lambda_c)|y_{1:n_c} \sim & \text{NormalGamma}\left(m^{*} = \frac{cm + n_c\bar{y}}{c + n_c}, c^{*} = c + n_c, a^{*} = a + \frac{n_c}{2}, b^{*} = b + \frac{1}{2}\sum_{i=1}^{n_c}(y_i - \bar{y})^2 + \frac{n_c c}{c + n_c}\frac{(\bar{y}-m)^2}{2}\right) \\
&= \text{NormalGamma}(11.8, 49, 24.5, 6344)
\end{aligned}
$$
   
## Corresponding Code   
   
   
```{r}
prior = data.frame(m = 0, c = 1, a = 0.5, b = 50)
findParam = function(prior, data){
  postParam = NULL
  c = prior$c
  m = prior$m
  a = prior$a
  b = prior$b
  n = length(data)
  postParam = data.frame(m = (c*m + n*mean(data))/(c + n), 
                c = c + n, 
                a = a + n/2, 
                b =  b + 0.5*(sum((data - mean(data))^2)) + 
                  (n*c *(mean(data)- m)^2)/(2*(c+n)))
  return(postParam)
}
postS = findParam(prior, x)
postC = findParam(prior, y)

```
```{r, results = 'asis', echo = FALSE}
xtable(rbind(prior = prior,
             `Spurters Posterior` = postS,
             `Controls Posterior` = postC), 
       caption = "Parameters")
```
### Task 3

Based on the calculations from the previous task, provide a scatterplot of samples from the posterior distributions for the two groups. What are your conclusions? 

```{r}
# sampling from two posteriors 

# Number of posterior simulations
sim = 1000

# initialize vectors to store samples
mus = NULL
lambdas = NULL
muc = NULL
lambdac = NULL

# Following formula from the NormalGamma with 
# the update paramaters accounted accounted for below 

lambdas = rgamma(sim, shape = postS$a, rate = postS$b)
lambdac = rgamma(sim, shape = postC$a, rate = postC$b)


mus = sapply(sqrt(1/(postS$c*lambdas)),rnorm, n = 1, mean = postS$m)
muc = sapply(sqrt(1/(postC$c*lambdac)),rnorm, n = 1, mean = postC$m)

# Store simulations
simDF = data.frame(lambda = c(lambdas, lambdac),
                   mu = c(mus, muc),
                   Treatment = rep(c("Spurters", "Controls"),
                                   each = sim))

simDF$lambda = simDF$lambda^{-0.5}

# Plot the simulations
ggplot(data = simDF, aes(x = mu, y = lambda, colour = Treatment, shape = Treatment)) +
  geom_point(alpha = 0.2) + 
  labs(x = expression(paste(mu, " (Mean Change in IQ Score)")),
       y = expression(paste(lambda^{-1/2}, " (Std. Dev. of Change)")))  + 
  ggtitle("Posterior Samples")+ 
  theme(plot.title = element_text(hjust = 0.5))
```

The simulated scatterplot does look similar to Figure 1 in that the control group is more concentrated with a smaller average mean change in IQ score, while the spurters group has a larger average mean change in IQ score.

# Task 4 (Finish for Homework)

Now, we can answer our original question: ``What is the posterior probability that $\mu_S>\mu_C$?'' 

The easiest way to do this is to take a bunch of samples from each of the posteriors, and see what fraction of times we have $\mu_S>\mu_C$. This is an example of a Monte Carlo approximation (much more to come on this in the future). 

To do this, we draw $N=10^6$ samples from each posterior:
\begin{align*}
&(\mu_S^{(1)},\lambda_S^{(1)}),\dotsc,(\mu_S^{(N)},\lambda_S^{(N)})\sim \NormalGamma(24.0,8,4,855.0)\\
&(\mu_C^{(1)},\lambda_C^{(1)}),\dotsc,(\mu_C^{(N)},\lambda_C^{(N)})\sim\NormalGamma(11.8,49,24.5,6344.0)
\end{align*}
and obtain the approximation
\begin{align*}
\Pr(\bm\mu_S > \bm\mu_C \mid x_{1:n_S},y_{1:n_C}) 
\approx \frac{1}{N} \sum_{i = 1}^N \I\big(\mu_S^{(i)}>\mu_C^{(i)}\big) =  \;\;??
\end{align*}

Interpret the posterior probability that you compute above. 

# Task 5 (Finish for Homework)

Let's return back to the prior assumptions. There are a few ways that you can check that the prior conforms with our prior beliefs. Let's go back and checks these. Draw some samples from the prior and look at them---this is probably the best general strategy. See Figure \ref{figure:pygmalion-prior}. It's also a good idea to look at sample hypothetical datasets $X_{1:n}$ drawn using these sampled parameter values. 

Please replicate a plot similar to Figure \ref{figure:pygmalion-prior} and report your findings. 

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/pygmalion-prior.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Samples of $(\mu,\sigma)$ from the prior.}
  \label{figure:pygmalion-prior}
\end{figure}

