---
title: "Lab 8.5: Review for Exam II"
author: "Olivier Binette"
date: "Friday October 16, 2020"
fontsize: 11pt
output: 
  beamer_presentation:
    include:
      in_header: preamble.tex
---

```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=4, fig.height=3, fig.align="center")
set.seed(1)
```

# Agenda

- Annoucements
- Review of gaussian mixture models
- Appendix: general review of sampling methods

#
\section{Announcements}

# Announcements

- Please vote!
  - Forgot to register? Not a problem! Bring an ID and proof of residence (e.g. bank statement or utility bill) to vote **before October 31** at an early voting site.
\pause

- Please fill out the TA section of your class evaluation!
  - Your evaluations are very important to me.

# 
\section{Review of gaussian mixture models}

# Review of gaussian mixture models



#
\section{Appendix: review of sampling methods}

# Review of sampling methods

1. Inverse CDF method
2. Rejection sampling
3. MCMC methods
    - Metropolis-Hastings
    - **Gibbs sampling**

# 1. Inverse CDF method

**Goal:** Generate samples $X_1, X_2, \dots, X_n$ from a distribution on $\mathbb{R}$ with CDF $F$.

**The trick:** If $F$ is invertible and $U \sim \text{Unif}(0,1)$, then $X = F^{-1}(U)$ has the correct distribution.

**When is it used?**
- Works only for *univariate* distributions.
- You need to be able to evaluate $F^{-1}$.

# 1. Inverse CDF method

**Example:** Sampling from an $\text{Exp}(\lambda)$ distribution

1. The CDF of $X \sim \text{Exp}(\lambda)$ is $F(x) = 1-e{-\lambda x}$.
2. Its inverse is $F^{-1}(u) = -\log(1-u)/\lambda$.

\small
```{r}
F.inv <- function(u, lambda=1) -log(1-u)/lambda

n = 1000
X = F.inv(runif(n))
```
\normalsize

# 1. Inverse CDF method

\small
```{r}
hist(X, prob=TRUE)
curve(dexp(x), add=TRUE)
```
\normalsize



# 2. Rejection sampling

**Goal:** Generate samples $X_1, X_2, \dots, X_n$ from a distribution with density (proportional to) $p(x)$.
\pause

**The trick:** Try to find a density $q(x)$ which you can sample from and such that $cq(x) \geq p(x)$ for some $c$.
\pause

**Algorithm:**

1. Generate $X \sim q(x)$ and $Y \sim \text{Unif}(0, c q(X))$.
2. If $Y < p(X)$, then return $X$. Otherwise go back to step 1.

# 2. Rejection sampling

**Example:**

Let $p(x) = \sin^2 (\pi x)$ be defined on $[0,1]$ and let $q(x) = 1$ for all $x$. Take $c = 1$ since $p(x) \leq 1$.
\pause

\small
```{r}
p <- function(x) sin(pi*x)^2
q <- Vectorize(function(x) 1)

# Vectorized form of rejection sampling:
k = 5000
X = runif(k) # Samples from q
Y = runif(k) # Samples uniform between 0 and cq(X)
X = X[Y < p(X)] # Only keep the X for which Y < p(X).

length(X)/5000 # Acceptance rate
```
\normalfont

# 2. Rejection sampling

\small
```{r}
hist(X, prob=TRUE, breaks=20)
curve(2*p(x), add=TRUE)
```
\normalfont

# 2. Rejection sampling

**When is rejection sampling used?**

- Works great for *univariate* densities (just like the inverse CDF method).
- You don't even need a normalizing constant for $p$ (e.g. posterior distributions!).
- Trickier for higher-dimensional distributions (that's where Gibbs sampling comes in).

# 3. Metropolis-Hastings

**Goal:** Generate a Markov Chain $X^{(1)}, X^{(2)}, \dots, X^{(n)}$ with stationary distribution (proportional to) $p(x)$.

  - In practice the $X^{(s)}$ are seen as correlated samples from the density proportional to $p(x)$.

**The trick:** 

- Given $X^{(s)}=x$, propose $X^{(s+1)} = x^*$ following some distribution $J(x^* \mid x)$. 
- Accept the proposal with probability
$$
  \alpha =\min \left\{1, \frac{p(x^*) J(x \mid x^*)}{p(x) J(x^* \mid x)} \right\},
$$
- Otherwise set $X^{(s+1)} = X^{(s)} = x$.

# Metropolis-Hastings

\url{https://gfycat.com/relievedglossyhowlermonkey}

# 3. Metropolis-Hastings

**When is it used?**

- To sample from high-dimensional distributions
- No need to know a normalizing constant for $p(x)$ (e.g. posterior distributions!).

**What to watch out for?**

- Convergence issues: you want your samples to be a good approximation to $p$ and to not be too correlated with one another.
- The acceptance rate of the proposals can help diagnose issues, but it doen't tell you about convergence.
- You need to look at convergence diagnostics.

# 4. Gibbs sampling

**Goal:** Generate a Markov Chain $X^{(1)}, X^{(2)}, \dots, X^{(n)}$ with stationary distribution (proportional to) $p(x)$, where $x = (x_1, x_2, \dots, x_k)$.

**The trick:** Reduce to sampling from the *full conditional distributions* $p(x_{i} \mid x_{(-i)})$.

**Algorithm:**

1. Initialize $X^{(1)} = (X^{(1)}_1, X^{(1)}_2, \dots, X^{(1)}_k)$ to fixed values.
2. For $s = 2, 3, \dots, n$, do:
  - $X^{(s)}_1 \sim p(x_1 \mid X^{(s-1)}_2, X^{(s-1)}_2, \dots, X^{(s-1)}_k)$
  - $X^{(s)}_2 \sim p(x_2 \mid X^{(s)}_1, X^{(s-1)}_3, \dots, X^{(s-1)}_k)$
  - $X^{(s)}_3 \sim p(x_3 \mid X^{(s)}_1, X^{(s)}_2, X^{(s-1)}_4,  \dots, X^{(s-1)}_k)$
  - $\vdots$
  - $X^{(s)}_k \sim p(x_k \mid X^{(s)}_1, X^{(s)}_2, \dots, X^{(s)}_{k-1})$

# 4. Gibbs sampling

**Example:** Go back to the gaussian mixture model example.

**When is it used?:**

- To sample from high-dimensional distributions
- No need to know a normalizing constant for $p(x)$ (e.g. posterior distributions!).
- You need to derive the full-posterior distributions.

**What to watch out for:**

- Convergence issues: you want your samples to be a good approximation to $p$ and to not be too correlated with one another.
- You need to look at convergence diagnostics.









